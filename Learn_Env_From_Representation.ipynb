{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/abyaadrafid/Representation_Learning_RL/blob/main/Learn_Env_From_Representation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j0-yoJ1TxVz1",
        "outputId": "a58fae8b-0fef-41d9-9f47-c97a812eb1fb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "libopenmpi-dev is already the newest version (2.1.1-8).\n",
            "swig is already the newest version (3.0.12-1).\n",
            "cmake is already the newest version (3.10.2-1ubuntu2.18.04.2).\n",
            "zlib1g-dev is already the newest version (1:1.2.11.dfsg-0ubuntu2.1).\n",
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-460\n",
            "Use 'apt autoremove' to remove it.\n",
            "0 upgraded, 0 newly installed, 0 to remove and 49 not upgraded.\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: stable-baselines[mpi]==2.10.0 in /usr/local/lib/python3.7/dist-packages (2.10.0)\n",
            "Requirement already satisfied: box2d in /usr/local/lib/python3.7/dist-packages (2.3.10)\n",
            "Requirement already satisfied: box2d-kengz in /usr/local/lib/python3.7/dist-packages (2.3.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from stable-baselines[mpi]==2.10.0) (1.21.6)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from stable-baselines[mpi]==2.10.0) (1.3.5)\n",
            "Requirement already satisfied: gym[atari,classic_control]>=0.11 in /usr/local/lib/python3.7/dist-packages (from stable-baselines[mpi]==2.10.0) (0.17.3)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from stable-baselines[mpi]==2.10.0) (3.2.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from stable-baselines[mpi]==2.10.0) (1.4.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from stable-baselines[mpi]==2.10.0) (1.1.0)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.7/dist-packages (from stable-baselines[mpi]==2.10.0) (4.1.2.30)\n",
            "Requirement already satisfied: cloudpickle>=0.5.5 in /usr/local/lib/python3.7/dist-packages (from stable-baselines[mpi]==2.10.0) (1.3.0)\n",
            "Requirement already satisfied: mpi4py in /usr/local/lib/python3.7/dist-packages (from stable-baselines[mpi]==2.10.0) (3.1.3)\n",
            "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from gym[atari,classic_control]>=0.11->stable-baselines[mpi]==2.10.0) (1.5.0)\n",
            "Requirement already satisfied: atari-py~=0.2.0 in /usr/local/lib/python3.7/dist-packages (from gym[atari,classic_control]>=0.11->stable-baselines[mpi]==2.10.0) (0.2.9)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.7/dist-packages (from gym[atari,classic_control]>=0.11->stable-baselines[mpi]==2.10.0) (7.1.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from atari-py~=0.2.0->gym[atari,classic_control]>=0.11->stable-baselines[mpi]==2.10.0) (1.15.0)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym[atari,classic_control]>=0.11->stable-baselines[mpi]==2.10.0) (0.16.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->stable-baselines[mpi]==2.10.0) (2.8.2)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->stable-baselines[mpi]==2.10.0) (3.0.9)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->stable-baselines[mpi]==2.10.0) (0.11.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->stable-baselines[mpi]==2.10.0) (1.4.3)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from kiwisolver>=1.0.1->matplotlib->stable-baselines[mpi]==2.10.0) (4.1.1)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas->stable-baselines[mpi]==2.10.0) (2022.1)\n"
          ]
        }
      ],
      "source": [
        "!apt install swig cmake libopenmpi-dev zlib1g-dev\n",
        "!pip install stable-baselines[mpi]==2.10.0 box2d box2d-kengz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YmRUFFzRrHMa",
        "outputId": "276de134-a3cd-4dac-9001-087f12eea7d4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "ZS1axk8kxZFf"
      },
      "outputs": [],
      "source": [
        "import gym\n",
        "from gym.spaces import Discrete\n",
        "import torch\n",
        "from collections import deque, defaultdict, namedtuple\n",
        "import random\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "from torch.optim import Adam\n",
        "from tqdm.notebook import tqdm\n",
        "import torch.nn.functional as F"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "CJXBax1A1EC_"
      },
      "outputs": [],
      "source": [
        "MAX_EPISODES = 1000\n",
        "MAX_EPISODE_LEN = 300\n",
        "BATCH_SIZE = 15\n",
        "EMBEDDING_SIZE = 128\n",
        "SEED = 0\n",
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oiDWij3-xgKL",
        "outputId": "c7e6912c-e373-4b6e-c723-3f9fb30e8ae0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Discrete(4)\n",
            "Box(-inf, inf, (8,), float32)\n"
          ]
        }
      ],
      "source": [
        "env = gym.make('LunarLander-v2')\n",
        "env.seed(0)\n",
        "print(env.action_space)\n",
        "print(env.observation_space)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### DQN network for trained agent"
      ],
      "metadata": {
        "id": "PllqUeWFZ160"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "KGQDhbBqt8D9"
      },
      "outputs": [],
      "source": [
        "class DQN(nn.Module):\n",
        "  def __init__(self, state_size, fc1_size, fc2_size, action_size, seed):\n",
        "    super(DQN, self).__init__()\n",
        "    self.seed = torch.manual_seed(seed)\n",
        "    self.layers = nn.Sequential(\n",
        "        nn.Linear(state_size, fc1_size),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(fc1_size, fc2_size),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(fc2_size, action_size)\n",
        "    )\n",
        "      \n",
        "  def forward(self, x):\n",
        "    return self.layers(x)    "
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Trained agent object"
      ],
      "metadata": {
        "id": "SZZ4bm55aHg-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "Q0BsE35qmCqG"
      },
      "outputs": [],
      "source": [
        "class TrainedAgent(nn.Module):\n",
        "  def __init__(self, path, state_size = env.observation_space.shape[0], fc1_size = 128, fc2_size = 256, action_size = env.action_space.n):\n",
        "    super(TrainedAgent, self).__init__()\n",
        "    self.network = DQN(state_size, fc1_size, fc2_size, action_size , 0)\n",
        "    self._load_weights(path)\n",
        "  \n",
        "  def _load_weights(self, path):\n",
        "    if torch.cuda.is_available() :\n",
        "      self.network.load_state_dict(torch.load(path))\n",
        "    else :\n",
        "      self.network.load_state_dict(torch.load(path, map_location=torch.device('cpu')))\n",
        "    self.network.eval()\n",
        "  \n",
        "  def act(self, state):\n",
        "    state = torch.tensor(state)\n",
        "    return np.argmax(self.network(state).cpu().data.numpy())"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Random agent for debugging and baseline"
      ],
      "metadata": {
        "id": "_932BeA5Z7O6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "WLdfCKDexiFW"
      },
      "outputs": [],
      "source": [
        "class RandomAgent() :\n",
        "  def __init__(self, seed : int, action_space : Discrete) :\n",
        "    self.seed = seed\n",
        "    self.action_space = action_space\n",
        "\n",
        "  def act(self, observation = None) :\n",
        "    return self.action_space.sample()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Experience collector\n",
        "Collects episodes using the given agent"
      ],
      "metadata": {
        "id": "zF5qdcHWaLax"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "KRSKdVWv3jxI"
      },
      "outputs": [],
      "source": [
        "collector_config = {\n",
        "    \"seed\" : 0,\n",
        "    \"env\" : env,\n",
        "    \"agent\" : \"trained\",\n",
        "    \"agent_weights_path\" : \"/content/drive/MyDrive/dqn_weights.pt\",\n",
        "    \"max_episodes\" : MAX_EPISODES,\n",
        "    \"max_episode_len\" : MAX_EPISODE_LEN,\n",
        "    \"action_space\" : env.action_space,\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "RySAK7cD1Fa3"
      },
      "outputs": [],
      "source": [
        "class ExperienceCollector():\n",
        "  def __init__(self, config : dict):\n",
        "    self.seed = config.get(\"seed\",0)\n",
        "    self.env = config.get(\"env\")\n",
        "    self.agent_type = config.get(\"agent\", \"random\")\n",
        "    self.max_episode_len = config.get(\"max_episode_len\", 300)\n",
        "    self.max_episodes = config.get(\"max_episodes\")\n",
        "    self.action_space = config.get(\"action_space\", Discrete(4))\n",
        "    self.memory = []\n",
        "    self.agent_weights = config.get(\"agent_weights_path\", None)\n",
        "\n",
        "    self.agent = self._make_agent()\n",
        "    self.current_episode = 0\n",
        "  \n",
        "  def __len__(self):\n",
        "    return len(self.memory)\n",
        "  \n",
        "  def _make_agent(self):\n",
        "    if self.agent_type == \"random\" :\n",
        "      return RandomAgent(self.seed, self.action_space)\n",
        "    elif self.agent_type == \"trained\" :\n",
        "      return TrainedAgent(self.agent_weights)\n",
        "\n",
        "  def add_episode(self, episode):\n",
        "    if self.current_episode >= self.max_episodes :\n",
        "      index = np.random.randint(0, self.max_episodes)\n",
        "      self.memory[index] = episode\n",
        "\n",
        "    else :\n",
        "      self.memory.append(episode)\n",
        "    self.current_episode +=1\n",
        "\n",
        "  def sample(self):\n",
        "    \"\"\"\n",
        "    Sample one episode from the memory, make tensors and move them to device\n",
        "    \"\"\"\n",
        "    states = torch.zeros((MAX_EPISODE_LEN,env.observation_space.shape[0]))\n",
        "    actions = torch.full((MAX_EPISODE_LEN, ), fill_value = -1, dtype = torch.float32)\n",
        "    rewards = torch.zeros((MAX_EPISODE_LEN))\n",
        "    next_states = torch.zeros((MAX_EPISODE_LEN,env.observation_space.shape[0]))\n",
        "    dones = torch.ones((MAX_EPISODE_LEN), dtype = torch.float32)\n",
        "\n",
        "    episode = random.sample(self.memory, k=1)\n",
        "    episode = np.array(episode, dtype=object).reshape(-1,5)\n",
        "\n",
        "    for index, step in enumerate(episode) :\n",
        "      states[index] = torch.from_numpy(step[0])\n",
        "      actions[index] = step[1]\n",
        "      rewards[index] = step[2]\n",
        "      next_states[index] = torch.from_numpy(step[3])\n",
        "      dones[index] = step[4]\n",
        "\n",
        "    return states.to(device), actions.to(device), rewards.to(device), next_states.to(device), dones.to(device)\n",
        "\n",
        "\n",
        "  def collect(self, num_episodes : int = 100, verbose = False) :\n",
        "    \"\"\"\n",
        "    collect the given number of episodes and store to memory\n",
        "    \"\"\"\n",
        "    for _ in range(num_episodes) :\n",
        "      current_episode = []\n",
        "      episode_length = 0\n",
        "      state = self.env.reset()\n",
        "      done = False\n",
        "\n",
        "      while not done :\n",
        "        if episode_length >= self.max_episode_len : break\n",
        "        action = self.agent.act(state)\n",
        "        next_state, reward, done, _ = self.env.step(action)\n",
        "\n",
        "        if done : done = 1 \n",
        "        else : done = 0\n",
        "\n",
        "        current_episode.append([state, action, reward, next_state, done])\n",
        "        episode_length +=1\n",
        "        \n",
        "        state = next_state\n",
        "      self.add_episode(current_episode)\n",
        "      \n",
        "    if verbose : \n",
        "      print(f'{num_episodes} episodes added to memory')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SEi9D0687VDs",
        "outputId": "3e8c3d18-b5eb-4c6f-a6b9-6f566969ef09"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "100 episodes added to memory\n"
          ]
        }
      ],
      "source": [
        "collector = ExperienceCollector(collector_config)\n",
        "collector.collect(100, verbose = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3hOxDaGCFwZU",
        "outputId": "e250e194-46ef-4799-c641-ce1bdd2b743f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "100"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "collector.__len__()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "Wy_ErY09dSuv"
      },
      "outputs": [],
      "source": [
        "_ = collector.sample()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Autoencoder for states"
      ],
      "metadata": {
        "id": "Q24FtpK2aqr3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "LqVRGxqq9CZ-"
      },
      "outputs": [],
      "source": [
        "class StateEncoder(nn.Module) :\n",
        "  def __init__(self, state_size : int, embedding_size : int, fc1_size : int = 64):\n",
        "    super(StateEncoder, self).__init__()\n",
        "    self.state_enc = nn.Sequential(\n",
        "        nn.Linear(state_size, fc1_size),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(fc1_size, embedding_size)\n",
        "    )\n",
        "  def forward(self, state):\n",
        "    return self.state_enc(state)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "k-rRDadpXRWE"
      },
      "outputs": [],
      "source": [
        "class StateDecoder(nn.Module) :\n",
        "  def __init__(self, state_size : int, embedding_size : int, fc1_size : int = 64):\n",
        "    super(StateDecoder, self).__init__()\n",
        "    self.state_dec = nn.Sequential(\n",
        "        nn.Linear(embedding_size, fc1_size),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(fc1_size, state_size)\n",
        "    )\n",
        "  def forward(self, state):\n",
        "    return self.state_dec(state)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Autoencoder for actions"
      ],
      "metadata": {
        "id": "M5LdsYNBata8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "rOFQLyy8FGo0"
      },
      "outputs": [],
      "source": [
        "class ActionEncoder(nn.Module) :\n",
        "  def __init__(self, embedding_size,fc1_size :int = 16) :\n",
        "    super(ActionEncoder, self).__init__()\n",
        "    self.action_enc = nn.Sequential(\n",
        "      nn.Linear(1, fc1_size),\n",
        "      nn.ReLU(),\n",
        "      nn.Linear(fc1_size, embedding_size)\n",
        "    )\n",
        "  def forward(self, action) :\n",
        "    return self.action_enc(action.unsqueeze(-1))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "tiWR3rU3QN7n"
      },
      "outputs": [],
      "source": [
        "class ActionDecoder(nn.Module) :\n",
        "  def __init__(self, embedding_size, fc1_size :int = 16) :\n",
        "    super(ActionDecoder, self).__init__()\n",
        "    self.action_dec = nn.Sequential(\n",
        "      nn.Linear(embedding_size, fc1_size),\n",
        "      nn.ReLU(),\n",
        "      nn.Linear(fc1_size, 1)\n",
        "    )\n",
        "  def forward(self, action) :\n",
        "    return self.action_dec(action).squeeze(-1)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model for reward prediction"
      ],
      "metadata": {
        "id": "ATTeQW2OawEr"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "tUrgakQWKwD-"
      },
      "outputs": [],
      "source": [
        "class RewardModel(nn.Module) :\n",
        "  def __init__(self, embedding_size=16, fc1_size = 8):\n",
        "    super(RewardModel, self).__init__()\n",
        "    self.layers = nn.Sequential(\n",
        "        nn.Linear(embedding_size*2, fc1_size),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(fc1_size, 1)\n",
        "    )\n",
        "  \n",
        "  def forward(self, x):\n",
        "    return self.layers(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model to predict end of episode"
      ],
      "metadata": {
        "id": "9pwqqPysa0eH"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "6KFx-8pRftL5"
      },
      "outputs": [],
      "source": [
        "class DonePredictor(nn.Module) :\n",
        "  def __init__(self, embedding_size=16, fc1_size = 8):\n",
        "    super(DonePredictor, self).__init__()\n",
        "    self.layers = nn.Sequential(\n",
        "        nn.Linear(embedding_size, fc1_size),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(fc1_size, 1),\n",
        "        nn.Sigmoid()\n",
        "    )\n",
        "  \n",
        "  def forward(self, x):\n",
        "    return self.layers(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### World model\n",
        "Consists of a recurrent state space model, a reward model and an episode finish predictor. Takes representation of state and action, uses a gru to learn the environment dynamics. GRU outputs are used for predicting :\n",
        "1. The next state\n",
        "2. Whether the state is last in the episode \n",
        "\n",
        "The reward model uses the representation of the previous state and the previous action and predicts rewards for the state action tuple."
      ],
      "metadata": {
        "id": "2XWF4XPca5NF"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "B6lDGTyaYXxC"
      },
      "outputs": [],
      "source": [
        "class WorldModel(nn.Module) :\n",
        "  def __init__(self, embedding_size, hidden_size= 16, state_size = env.observation_space.shape[0]) :\n",
        "    super(WorldModel, self).__init__()\n",
        "    self.hidden_size = hidden_size\n",
        "    self.gru = nn.GRU(input_size = embedding_size*2, hidden_size = hidden_size)\n",
        "    self.state_predictor = nn.Sequential(\n",
        "        nn.Linear(hidden_size, embedding_size),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(embedding_size, state_size)\n",
        "    )\n",
        "    self.reward_model = RewardModel(embedding_size)\n",
        "    self.done_model = DonePredictor(hidden_size)\n",
        "    self.init_hidden()\n",
        "    \n",
        "  def init_hidden(self):\n",
        "    self.hidden = torch.zeros(1, self.hidden_size).to(device)\n",
        "  \n",
        "  def forward(self, encoded_states, actions):\n",
        "    inputs = torch.cat([encoded_states, actions], dim = -1)\n",
        "    output, self.hidden = self.gru(inputs, self.hidden)\n",
        "    rewards = self.reward_model(inputs)\n",
        "    dones = self.done_model(output)\n",
        "    state = self.state_predictor(output)\n",
        "\n",
        "    return state, rewards.squeeze(-1), dones.squeeze(-1)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Initialize the models"
      ],
      "metadata": {
        "id": "xivK3wFhbv8B"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "RZmpwYqMYCrl"
      },
      "outputs": [],
      "source": [
        "state_encoder = StateEncoder(env.observation_space.shape[0], EMBEDDING_SIZE).to(device).train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "_oL6jNSzXvwO"
      },
      "outputs": [],
      "source": [
        "state_decoder = StateDecoder(env.observation_space.shape[0], EMBEDDING_SIZE).to(device).train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "vGqpvuDuFkPc"
      },
      "outputs": [],
      "source": [
        "action_encoder = ActionEncoder(EMBEDDING_SIZE).to(device).train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "VApbCJFvRYYr"
      },
      "outputs": [],
      "source": [
        "action_decoder = ActionDecoder(EMBEDDING_SIZE).to(device).train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "ncVyhX_NaEYO"
      },
      "outputs": [],
      "source": [
        "wm = WorldModel(EMBEDDING_SIZE, hidden_size = 16, state_size = env.observation_space.shape[0]).to(device).train()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Multitask Loss\n",
        "Our world model loss is a multi task loss. Different losses have different magnitudes, as a result one loss dominates the others. For example, the reward loss is very high at the beginning, and it make the training process unstable.\n",
        "\n",
        "So we learn weights for different losses."
      ],
      "metadata": {
        "id": "f4OT6TBEbzIv"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "44KJBeQCNfYY"
      },
      "outputs": [],
      "source": [
        "class MultiTaskLoss(nn.Module) :\n",
        "  def __init__(self, num_losses):\n",
        "    super(MultiTaskLoss, self).__init__()\n",
        "\n",
        "    self.num_losees = num_losses\n",
        "    self.log_vars = nn.Parameter(torch.zeros(num_losses))\n",
        "    self.transition_loss = nn.MSELoss()\n",
        "    self.reward_loss = nn.MSELoss()\n",
        "    self.done_loss = nn.MSELoss()\n",
        "  \n",
        "  def forward(self, preds, targets) :\n",
        "    t_loss = self.transition_loss(preds[0], targets[0])\n",
        "    r_loss = self.reward_loss(preds[1], targets[1])\n",
        "    d_loss = self.done_loss(preds[2], targets[2])\n",
        "\n",
        "    t_loss = torch.exp(-self.log_vars[0])*t_loss + self.log_vars[0]\n",
        "    r_loss = torch.exp(-self.log_vars[1])*r_loss + self.log_vars[1]\n",
        "    d_loss = torch.exp(-self.log_vars[2])*d_loss + self.log_vars[2]\n",
        "\n",
        "    return t_loss, r_loss, d_loss\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Loss definitions\n",
        "We use two MSELosses for state and action autoencoders, and our trainable multitask loss for the world model\n",
        "\n",
        "We use adam optimizers for all of our different losses."
      ],
      "metadata": {
        "id": "eB3rQQPpg9Q5"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "71FAwOnumTCn"
      },
      "outputs": [],
      "source": [
        "state_reconstruction_loss = nn.MSELoss()\n",
        "action_reconstruction_loss = nn.MSELoss()\n",
        "wm_loss = MultiTaskLoss(num_losses = 3).to(device).train()\n",
        "wm_optimizer = Adam(list(wm.parameters())+list(wm_loss.parameters()))\n",
        "state_optimizer = Adam(list(state_encoder.parameters()) + list(state_decoder.parameters()), lr = 1e-5)\n",
        "action_optimizer = Adam(list(action_encoder.parameters()) + list(action_decoder.parameters()), lr = 5e-6)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "Wnuo22gxQL-p"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "import statistics"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Learning env loop\n",
        "1. Sample one episode from memory\n",
        "2. Use state and action autoencoder to learn representation on the embedding size\n",
        "3. Use encoded states and encoded actions as input for the state space model\n",
        "4. State space model predicts the next state, reward and done \n",
        "5. Get loss for each component\n",
        "6. Run backward"
      ],
      "metadata": {
        "id": "cbDlMrgliTWA"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "XoiTbl2PXN4c"
      },
      "outputs": [],
      "source": [
        "def wm_learn_env(num_episodes, collect_every = 500, collect_number = 500, print_every = 50) :\n",
        "  t_losses, r_losses, d_losses, s_losses, a_losses, losses = [], [], [], [], [], []\n",
        "  t_buffer, r_buffer, d_buffer, s_buffer, a_buffer, buffer = [], [], [], [] ,[], []\n",
        "\n",
        "  for episode in tqdm(range(num_episodes)) :\n",
        "    state_encoder.zero_grad()\n",
        "    action_encoder.zero_grad()\n",
        "    state_decoder.zero_grad()\n",
        "    action_decoder.zero_grad()\n",
        "    wm.init_hidden()\n",
        "    wm.zero_grad()\n",
        "    states, actions, rewards, next_states, dones = collector.sample()\n",
        "\n",
        "    if states is None : return \n",
        "\n",
        "    encoded_states = state_encoder(states)\n",
        "    decoded_states = state_decoder(encoded_states)\n",
        "    encoded_actions = action_encoder(actions)\n",
        "    decoded_actions = action_decoder(encoded_actions)\n",
        "\n",
        "    predicted_next_states, predicted_rewards, predicted_dones = wm(encoded_states, encoded_actions)\n",
        "\n",
        "    t_loss, r_loss, d_loss = wm_loss([predicted_next_states, predicted_rewards, predicted_dones],[next_states, rewards, dones])\n",
        "    state_loss = state_reconstruction_loss(decoded_states, states)\n",
        "    action_loss = action_reconstruction_loss(decoded_actions, actions)\n",
        "\n",
        "    model_loss = t_loss+r_loss+d_loss\n",
        "    loss = model_loss + state_loss + action_loss\n",
        "\n",
        "    loss.backward()\n",
        "    losses.append(loss.item())\n",
        "    t_losses.append(t_loss.item())\n",
        "    t_buffer.append(t_loss.item())\n",
        "\n",
        "    r_losses.append(r_loss.item())\n",
        "    r_buffer.append(r_loss.item())\n",
        "\n",
        "    d_losses.append(d_loss.item())\n",
        "    d_buffer.append(d_loss.item())\n",
        "\n",
        "    s_losses.append(state_loss.item())\n",
        "    s_buffer.append(state_loss.item())\n",
        "\n",
        "    a_losses.append(action_loss.item())\n",
        "    a_buffer.append(action_loss.item())\n",
        "    \n",
        "    wm_optimizer.step()\n",
        "    state_optimizer.step()\n",
        "    action_optimizer.step()\n",
        "\n",
        "    if episode % print_every == 0 :\n",
        "      print(f'Episode {episode} :\\n\\\n",
        "      Avg Transition_loss : {statistics.mean(t_buffer)} \\n\\\n",
        "      Avg Reward_loss : {statistics.mean(r_buffer)}\\n\\\n",
        "      Avg End loss : {statistics.mean(d_buffer)}\\n\\\n",
        "      Avg State Reconstruction loss : {statistics.mean(s_buffer)}\\n\\\n",
        "      Avg Action Reconstruction loss : {statistics.mean(a_buffer)}'\n",
        "      )\n",
        "      t_buffer, r_buffer, d_buffer, s_buffer, a_buffer,buffer = [], [], [], [] ,[], []\n",
        "\n",
        "    if episode % collect_every == 0 :\n",
        "      collector.collect(collect_number, verbose=False)\n",
        "\n",
        "  return t_losses, r_losses, d_losses,s_losses, a_losses ,losses"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153,
          "referenced_widgets": [
            "897b24376fb340979379b3a1530083a8",
            "267af7a2cff141bdab26df81b33c88e2",
            "6abc4e62cce64d2082d0800ae6c40206",
            "26c720918f5b4e5281a72e7bc870c505",
            "1d36b1f7c3864699a692f568a662c7d6",
            "cb95c910bc884d479fa1787bcdb2c06b",
            "48b4484a132049b7a9c1aadb09d6103d",
            "4ea5a055979c4495a3dd9d45ced32a68",
            "414de0ebe92d4d799c46a7bdd15abe4a",
            "edc4b984553e4902b9bb0e4729893f52",
            "326fec90ce3545588396ee8333fadf96"
          ]
        },
        "id": "8lqjGkH3MCdr",
        "outputId": "a0cb4ca6-d0b1-452e-83a7-c007a4cf03de"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/100000 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "897b24376fb340979379b3a1530083a8"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode 0 :\n",
            "      Avg Transition_loss : 0.19877824187278748 \n",
            "      Avg Reward_loss : 37.96870040893555\n",
            "      Avg End loss : 0.2749415338039398\n",
            "      Avg State Reconstruction loss : 0.12970134615898132\n",
            "      Avg Action Reconstruction loss : 1.9640289545059204\n"
          ]
        }
      ],
      "source": [
        "t_loss, r_loss, d_loss, s_loss, a_loss ,loss = wm_learn_env(100000, print_every=5000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yu2MPx-AR5Fe"
      },
      "outputs": [],
      "source": [
        "statistics.mean(loss)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gkwNzAWWv-7_"
      },
      "outputs": [],
      "source": [
        "agent = TrainedAgent('/content/drive/MyDrive/dqn_weights.pt').to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xLOeEAfvbvLi"
      },
      "outputs": [],
      "source": [
        "def run_episode(state, agent, num_steps =300):\n",
        "  done = False\n",
        "  steps = 0\n",
        "  rewards = 0\n",
        "  for _ in tqdm(range(num_steps)) :\n",
        "    action = agent.act(state)\n",
        "    action = torch.tensor(action, dtype=torch.float32).unsqueeze(0).to(device)\n",
        "    encoded_action = action_encoder(action)\n",
        "    encoded_state = state_encoder(state)\n",
        "\n",
        "    next_state, reward, done = wm(encoded_action, encoded_state)\n",
        "    state = next_state.clone().detach()\n",
        "\n",
        "    rewards += reward.detach().item()\n",
        "    if bool(done.argmax().item()) : break\n",
        "  \n",
        "  return rewards\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KyawjERehLbz"
      },
      "outputs": [],
      "source": [
        "run_episode(torch.Tensor(env.reset()).reshape(1,-1).to(device), agent)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pm9U0h6pTrfU"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "Learn_Env_From_Representation.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyP064lvienHO1yewg9CtKc+",
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "897b24376fb340979379b3a1530083a8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_267af7a2cff141bdab26df81b33c88e2",
              "IPY_MODEL_6abc4e62cce64d2082d0800ae6c40206",
              "IPY_MODEL_26c720918f5b4e5281a72e7bc870c505"
            ],
            "layout": "IPY_MODEL_1d36b1f7c3864699a692f568a662c7d6"
          }
        },
        "267af7a2cff141bdab26df81b33c88e2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cb95c910bc884d479fa1787bcdb2c06b",
            "placeholder": "​",
            "style": "IPY_MODEL_48b4484a132049b7a9c1aadb09d6103d",
            "value": "  2%"
          }
        },
        "6abc4e62cce64d2082d0800ae6c40206": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4ea5a055979c4495a3dd9d45ced32a68",
            "max": 100000,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_414de0ebe92d4d799c46a7bdd15abe4a",
            "value": 1638
          }
        },
        "26c720918f5b4e5281a72e7bc870c505": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_edc4b984553e4902b9bb0e4729893f52",
            "placeholder": "​",
            "style": "IPY_MODEL_326fec90ce3545588396ee8333fadf96",
            "value": " 1638/100000 [03:39&lt;31:10, 52.60it/s]"
          }
        },
        "1d36b1f7c3864699a692f568a662c7d6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cb95c910bc884d479fa1787bcdb2c06b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "48b4484a132049b7a9c1aadb09d6103d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4ea5a055979c4495a3dd9d45ced32a68": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "414de0ebe92d4d799c46a7bdd15abe4a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "edc4b984553e4902b9bb0e4729893f52": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "326fec90ce3545588396ee8333fadf96": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}